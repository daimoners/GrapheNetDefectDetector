{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pathlib import Path\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from lib.lib_utils import Utils\n",
    "    import seaborn as sns\n",
    "    from lib.lib_defect_analysis import Features\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    from tqdm import tqdm\n",
    "    from matplotlib.ticker import MaxNLocator\n",
    "    import cv2\n",
    "    import json\n",
    "    import pywt\n",
    "    import xgboost as xgb\n",
    "    import lightgbm as lgb\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    import shap\n",
    "except Exception as e:\n",
    "    print(f\"Some module are missing: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path().resolve().joinpath(\"data_234\")\n",
    "xyz_files_path = data_path.joinpath(\"xyz_files\")\n",
    "yolo_model_path = data_path.joinpath(\"models\", \"best.pt\")\n",
    "images_path = data_path.joinpath(\"images\")\n",
    "crops_path = data_path.joinpath(\"crops\")\n",
    "plot_path = data_path.joinpath(\"plots\")\n",
    "pred_path = data_path.joinpath(\"predictions\")\n",
    "\n",
    "plot_path.mkdir(exist_ok=True, parents=True)\n",
    "pred_path.mkdir(exist_ok=True, parents=True)\n",
    "plt.style.use(\"seaborn-v0_8-paper\")\n",
    "colors = [\"#F0741E\", \"#276CB3\"]\n",
    "\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", [colors[1], colors[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dim = [39.53476932, 34.27629786]\n",
    "Utils.from_xyz_to_png(xyz_files_path, images_path, max_dim=max_dim, multiplier=6)\n",
    "\n",
    "Utils.generate_yolo_crops(\n",
    "    images_path,\n",
    "    crops_path,\n",
    "    yolo_model_path,\n",
    "    binary_mask=True,\n",
    "    device=\"cpu\",\n",
    "    confidence=0.75,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(data: np.ndarray, bins: int = 256) -> float:\n",
    "    data_flat = data.flatten()\n",
    "    histogram, _ = np.histogram(data_flat, bins=bins, range=(0, bins), density=True)\n",
    "    entropy = -np.sum(histogram * np.log2(histogram + 1e-10))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def clalc_shift_spect(img):\n",
    "    f = np.fft.fft2(img)\n",
    "\n",
    "    coeffs = pywt.wavedec2(img, \"haar\", level=2)\n",
    "    return f, coeffs\n",
    "\n",
    "\n",
    "def extract_frequency_features(\n",
    "    image: Path | np.ndarray, wavelet: str = \"db4\", bins: int = 256\n",
    ") -> dict:\n",
    "    image = image.resolve()\n",
    "    if isinstance(image, Path):\n",
    "        img = cv2.imread(str(image), cv2.IMREAD_GRAYSCALE)\n",
    "    else:\n",
    "        img = image.copy()\n",
    "        if len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # f = np.fft.fft2(img)\n",
    "    f, coeffs = clalc_shift_spect(img)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1e-10)\n",
    "\n",
    "    mean_freq = np.mean(magnitude_spectrum)\n",
    "    std_freq = np.std(magnitude_spectrum)\n",
    "    max_freq = np.max(magnitude_spectrum)\n",
    "    min_freq = np.min(magnitude_spectrum)\n",
    "    median_freq = np.median(magnitude_spectrum)\n",
    "    energy = np.sum(np.abs(fshift) ** 2)\n",
    "\n",
    "    rows, cols = magnitude_spectrum.shape\n",
    "    crow, ccol = rows // 2, cols // 2\n",
    "\n",
    "    histogram, _ = np.histogram(\n",
    "        magnitude_spectrum, bins=bins, range=(0, bins), density=True\n",
    "    )\n",
    "    entropy = -np.sum(histogram * np.log2(histogram + 1e-10))\n",
    "\n",
    "    low_freq_energy = np.sum(magnitude_spectrum[:crow, :ccol])\n",
    "    high_freq_energy = np.sum(magnitude_spectrum[crow:, ccol:])\n",
    "    frequency_contrast = high_freq_energy - low_freq_energy\n",
    "\n",
    "    # coeffs = pywt.wavedec2(img, wavelet, level=2)\n",
    "    cA2, (cH2, cV2, cD2), (cH1, cV1, cD1) = coeffs\n",
    "\n",
    "    wavelet_features = {\n",
    "        \"wavelet_energy_A2\": np.sum(np.square(cA2)),\n",
    "        \"wavelet_energy_H2\": np.sum(np.square(cH2)),\n",
    "        \"wavelet_energy_V2\": np.sum(np.square(cV2)),\n",
    "        \"wavelet_energy_D2\": np.sum(np.square(cD2)),\n",
    "        \"wavelet_std_A2\": np.std(cA2),\n",
    "        \"wavelet_std_H2\": np.std(cH2),\n",
    "        \"wavelet_std_V2\": np.std(cV2),\n",
    "        \"wavelet_std_D2\": np.std(cD2),\n",
    "        \"wavelet_energy_H1\": np.sum(np.square(cH1)),\n",
    "        \"wavelet_energy_V1\": np.sum(np.square(cV1)),\n",
    "        \"wavelet_energy_D1\": np.sum(np.square(cD1)),\n",
    "        \"wavelet_std_H1\": np.std(cH1),\n",
    "        \"wavelet_std_V1\": np.std(cV1),\n",
    "        \"wavelet_std_D1\": np.std(cD1),\n",
    "        \"wavelet_entropy_A2\": calculate_entropy(cA2, bins),\n",
    "        \"wavelet_entropy_H2\": calculate_entropy(cH2, bins),\n",
    "        \"wavelet_entropy_V2\": calculate_entropy(cV2, bins),\n",
    "        \"wavelet_entropy_D2\": calculate_entropy(cD2, bins),\n",
    "        \"wavelet_entropy_H1\": calculate_entropy(cH1, bins),\n",
    "        \"wavelet_entropy_V1\": calculate_entropy(cV1, bins),\n",
    "        \"wavelet_entropy_D1\": calculate_entropy(cD1, bins),\n",
    "    }\n",
    "\n",
    "    frequency_features = {\n",
    "        \"mean_frequency\": mean_freq,\n",
    "        \"std_frequency\": std_freq,\n",
    "        \"max_frequency\": max_freq,\n",
    "        \"min_frequency\": min_freq,\n",
    "        \"median_frequency\": median_freq,\n",
    "        \"energy\": energy,\n",
    "        \"entropy\": entropy,\n",
    "        \"low_frequency_energy\": low_freq_energy,\n",
    "        \"high_frequency_energy\": high_freq_energy,\n",
    "        \"frequency_contrast\": frequency_contrast,\n",
    "    }\n",
    "\n",
    "    frequency_features.update(wavelet_features)\n",
    "\n",
    "    return frequency_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [\n",
    "    \"area\",\n",
    "    \"perimeter\",\n",
    "    \"circularity\",\n",
    "    \"solidity\",\n",
    "    \"compactness\",\n",
    "    \"feret_diameter\",\n",
    "    \"edge_density\",\n",
    "    \"eccentricity\",\n",
    "    \"number_of_edges\",\n",
    "    \"GLCM_energy\",\n",
    "    \"GLCM_correlation\",\n",
    "    \"GLCM_homogeneity\",\n",
    "    \"GLCM_energy\",\n",
    "    \"GLCM_contrast\",\n",
    "    \"max_frequency\",\n",
    "    \"energy\",\n",
    "    \"entropy\",\n",
    "    \"std_frequency\",\n",
    "    \"min_frequency\",\n",
    "    \"mean_frequency\",\n",
    "    \"median_frequency\",\n",
    "    \"low_frequency_energy\",\n",
    "    \"high_frequency_energy\",\n",
    "    \"frequency_contrast\",\n",
    "    \"wavelet_energy_A2\",\n",
    "    \"wavelet_energy_H2\",\n",
    "    \"wavelet_energy_V2\",\n",
    "    \"wavelet_energy_D2\",\n",
    "    \"wavelet_std_A2\",\n",
    "    \"wavelet_std_H2\",\n",
    "    \"wavelet_std_V2\",\n",
    "    \"wavelet_std_D2\",\n",
    "    \"wavelet_energy_H1\",\n",
    "    \"wavelet_energy_V1\",\n",
    "    \"wavelet_energy_D1\",\n",
    "    \"wavelet_std_H1\",\n",
    "    \"wavelet_std_V1\",\n",
    "    \"wavelet_std_D1\",\n",
    "    \"wavelet_entropy_A2\",\n",
    "    \"wavelet_entropy_H2\",\n",
    "    \"wavelet_entropy_V2\",\n",
    "    \"wavelet_entropy_D2\",\n",
    "    \"wavelet_entropy_H1\",\n",
    "    \"wavelet_entropy_V1\",\n",
    "    \"wavelet_entropy_D1\",\n",
    "]\n",
    "\n",
    "\n",
    "target_list = [\n",
    "    \"fermi_level_ev\",\n",
    "    \"IP_ev\",\n",
    "    \"EA_ev\",\n",
    "    \"band_gap_ev\",\n",
    "    \"energy_per_atom\",\n",
    "    \"current\",\n",
    "]\n",
    "\n",
    "target_labels = {\n",
    "    \"fermi_level_ev\": (\"Fermi Level [eV] - predicted\", \"Fermi Level[eV] - true\"),\n",
    "    \"EA_ev\": (\n",
    "        \"Ionization Potential [eV] - predicted\",\n",
    "        \"Ionization Potential [eV] - true\",\n",
    "    ),\n",
    "    \"IP_ev\": (\"Electron Affinity [eV] - predicted\", \"Electron Affinity [eV] - true\"),\n",
    "    \"band_gap_ev\": (\"Band Gap [eV] - predicted\", \"Band Gap [eV] - true\"),\n",
    "    \"energy_per_atom\": (\n",
    "        \"Energy Per Atom [eV] - predicted\",\n",
    "        \"Energy Per Atom [eV] - true\",\n",
    "    ),\n",
    "    \"current\": (\n",
    "        \"Current [μA] - predicted\",\n",
    "        \"Current [μA] - true\",\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = data_path.joinpath(\"features.csv\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not filepath:\n",
    "    images = [\n",
    "        f for f in crops_path.iterdir() if f.suffix.lower() in Features.IMAGE_EXTENSIONS\n",
    "    ]\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    for image in tqdm(images):\n",
    "        if len(features) == 0:\n",
    "            keys_list = list(features.keys())\n",
    "            df = pd.DataFrame(columns=keys_list)\n",
    "        else:\n",
    "            features.clear()\n",
    "\n",
    "        name = image.stem.split(\"_crop\")[0]\n",
    "\n",
    "        with open(str(xyz_files_path.joinpath(f\"{name}.xyz\")), \"r\") as file:\n",
    "            first_line = file.readline()\n",
    "        n_atoms = int(first_line.strip())\n",
    "\n",
    "        features.update({\"file_name\": name})\n",
    "        features.update({\"n_atoms\": n_atoms})\n",
    "\n",
    "        shape_features = Features.extract_shape_features(image, grayscale=True)\n",
    "        if shape_features is not None:\n",
    "            features.update(shape_features)\n",
    "\n",
    "        edge_features = Features.extract_edge_features(image, grayscale=True)\n",
    "        if edge_features is not None:\n",
    "            features.update(edge_features)\n",
    "\n",
    "        texture_features = Features.extract_texture_features(image)\n",
    "        if texture_features is not None:\n",
    "            features.update(texture_features)\n",
    "\n",
    "        # frequency_features = Features.extract_frequency_features(image)\n",
    "        # if frequency_features is not None:\n",
    "        #     features.update(frequency_features)\n",
    "\n",
    "        new_row = pd.Series(features)\n",
    "        df = pd.concat([df, new_row.to_frame().T], ignore_index=True)\n",
    "        df[\"file_name\"] = df[\"file_name\"].str.replace(\"_opt\", \"\", regex=False)\n",
    "    grouped_df = (\n",
    "        df.groupby(\"file_name\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"n_atoms\": \"first\",\n",
    "                \"area\": \"sum\",\n",
    "                \"num_pixels\": \"sum\",\n",
    "                \"perimeter\": \"sum\",\n",
    "                \"circularity\": lambda x: (x * df.loc[x.index, \"area\"]).sum()\n",
    "                / df.loc[x.index, \"area\"].sum(),\n",
    "                \"solidity\": lambda x: (x * df.loc[x.index, \"area\"]).sum()\n",
    "                / df.loc[x.index, \"area\"].sum(),\n",
    "                \"compactness\": lambda x: (x * df.loc[x.index, \"area\"]).sum()\n",
    "                / df.loc[x.index, \"area\"].sum(),\n",
    "                \"feret_diameter\": lambda x: (x * df.loc[x.index, \"area\"]).sum()\n",
    "                / df.loc[x.index, \"area\"].sum(),\n",
    "                \"eccentricity\": lambda x: (x * df.loc[x.index, \"area\"]).sum()\n",
    "                / df.loc[x.index, \"area\"].sum(),\n",
    "                \"number_of_edges\": lambda x: (x * df.loc[x.index, \"area\"]).sum()\n",
    "                / df.loc[x.index, \"area\"].sum(),\n",
    "                \"edge_density\": lambda x: (x * df.loc[x.index, \"area\"]).sum()\n",
    "                / df.loc[x.index, \"area\"].sum(),\n",
    "                \"GLCM_contrast\": lambda x: (x * df.loc[x.index, \"area\"]).sum()\n",
    "                / df.loc[x.index, \"area\"].sum(),\n",
    "                \"GLCM_homogeneity\": lambda x: (x * df.loc[x.index, \"area\"]).sum()\n",
    "                / df.loc[x.index, \"area\"].sum(),\n",
    "                \"GLCM_energy\": lambda x: (x * df.loc[x.index, \"area\"]).sum()\n",
    "                / df.loc[x.index, \"area\"].sum(),\n",
    "                \"GLCM_correlation\": lambda x: (x * df.loc[x.index, \"area\"]).sum()\n",
    "                / df.loc[x.index, \"area\"].sum(),\n",
    "            }\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    #\n",
    "    original_df = pd.read_csv(xyz_files_path.joinpath(\"target_graphene_dftb.csv\"))\n",
    "\n",
    "    energy_dict = original_df.set_index(\"file_name\")[\"total_energy_eV\"].to_dict()\n",
    "    fermi_dict = original_df.set_index(\"file_name\")[\"fermi_level_ev\"].to_dict()\n",
    "    ip_dict = original_df.set_index(\"file_name\")[\"IP_ev\"].to_dict()\n",
    "    ea_dict = original_df.set_index(\"file_name\")[\"EA_ev\"].to_dict()\n",
    "    band_gap_dict = original_df.set_index(\"file_name\")[\"band_gap_ev\"].to_dict()\n",
    "    current_dict = original_df.set_index(\"file_name\")[\"current\"].to_dict()\n",
    "    flake_total_area_dict = original_df.set_index(\"file_name\")[\n",
    "        \"flake_total_area\"\n",
    "    ].to_dict()\n",
    "\n",
    "    # Aggiunta della colonna total_energy al primo dataframe\n",
    "    grouped_df[\"total_energy_eV\"] = grouped_df[\"file_name\"].map(energy_dict)\n",
    "    grouped_df[\"fermi_level_ev\"] = grouped_df[\"file_name\"].map(fermi_dict)\n",
    "    grouped_df[\"IP_ev\"] = grouped_df[\"file_name\"].map(ip_dict)\n",
    "    grouped_df[\"EA_ev\"] = grouped_df[\"file_name\"].map(ea_dict)\n",
    "    grouped_df[\"band_gap_ev\"] = grouped_df[\"file_name\"].map(band_gap_dict)\n",
    "    grouped_df[\"energy_per_atom\"] = (\n",
    "        grouped_df[\"total_energy_eV\"] / grouped_df[\"n_atoms\"]\n",
    "    )\n",
    "    grouped_df[\"current\"] = grouped_df[\"file_name\"].map(current_dict)\n",
    "    grouped_df[\"flake_total_area\"] = grouped_df[\"file_name\"].map(flake_total_area_dict)\n",
    "    grouped_df = grouped_df.dropna()\n",
    "\n",
    "    grouped_df.to_csv(data_path.joinpath(\"features.csv\"), index=False)\n",
    "else:\n",
    "    grouped_df = pd.read_csv(data_path.joinpath(\"features.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_dict = {}\n",
    "for filename in tqdm(grouped_df[\"file_name\"].to_list()):\n",
    "    img = images_path.joinpath(f\"{filename}_opt.png\")\n",
    "    dict_img = extract_frequency_features(img)\n",
    "    frequency_dict[filename] = dict_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df[\"current\"] = grouped_df[\"current\"] * 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_df = pd.DataFrame.from_dict(frequency_dict, orient=\"index\")\n",
    "grouped_df = grouped_df.join(frequency_df, on=\"file_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_remove = grouped_df[\n",
    "    (grouped_df[\"current\"] >= grouped_df[\"current\"].min())\n",
    "    & (grouped_df[\"current\"] <= 9e-02)\n",
    "].index\n",
    "\n",
    "# Rimuovere le righe corrispondenti agli indici trovati\n",
    "grouped_df = grouped_df.drop(indices_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grouped_df.isna().any().any():\n",
    "    print(\"Avviso: Il DataFrame contiene valori NaN.\")\n",
    "else:\n",
    "    print(\"Il DataFrame non contiene valori NaN.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_gdd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
